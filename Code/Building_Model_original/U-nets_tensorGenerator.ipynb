{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative between different models for TED Talks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:18:53.569955Z",
     "start_time": "2019-05-21T13:18:53.547040Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Libraries_functions import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads data from folders directly to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T14:08:49.821465Z",
     "start_time": "2019-05-20T14:01:22.073253Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo los nombres de los wav de sus directorios ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Load original load function\n",
    "#df_data,df_route = load_df('D:\\Caracuel\\WaveNet-seaBackGroundNoise\\Datasets\\Data\\dataframe_large.csv')\n",
    "#df_data,df_route = load_df('D:\\Caracuel\\WaveNet-seaBackGroundNoise\\Code\\generatingData\\dataframe.csv')\n",
    "#df_data,df_route = load_df('./dataframe.csv')\n",
    "\n",
    "##Load data with read wavs\n",
    "df_data=read_wavs(\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Noise\\\\Chunked_large\\\\\",\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Audio_mezclado_large\\\\\")\n",
    "#df_data=read_wavs(\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Pruebas_pequeñas_noise\\\\\",\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Pruebas_pequeñas_mixed\\\\\")\n",
    "\n",
    "\n",
    "##Load paths of data for generator\n",
    "#df_data=read_path(\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Noise\\\\Chunked_large\\\\\",\"D:\\\\Caracuel\\\\WaveNet-seaBackGroundNoise\\\\Datasets\\\\Audio_mezclado_large\\\\\")\n",
    "#create train, validate and test datasets\n",
    "#train, validate, test = np.split(df_data.sample(frac=1), [int(.6*len(df_data)), int(.8*len(df_data))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "miramos cual es la longitud más pequeña por si se ha descuadrado algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_length = min(map(len, df_data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos solo los datos de lluvia para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# df_lluvia=pd.DataFrame()\n",
    "# lluvia_mezclada=[]\n",
    "# lluvia=[]\n",
    "# for i in tqdm(range(len(df_data))):\n",
    "#     if(i>=df_data.shape[0]/4-1 and i<df_data.shape[0]/4+499-1):\n",
    "#         lluvia.append(df_data.data[i])\n",
    "#         lluvia_mezclada.append(df_data.noise_original[i])\n",
    "\n",
    "# df_lluvia['data']=lluvia\n",
    "# df_lluvia['noise_original']=lluvia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wave U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets prepare the train and test data for the model.\n",
    "\n",
    "For the Wave U-Net model, all files are going to be chuncked to 32k frames due to the model input and output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos cuanto tiene que medir la red en funcion del tamaño de la entrada. Ajustaremos dicho dato lo más posible a la entrada original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=shortest_length-shortest_length%16\n",
    "\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T15:45:55.159007Z",
     "start_time": "2019-05-21T15:45:54.970536Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = chunk4unet(df_data, 'data', 'noise_original',inp)\n",
    "\n",
    "del df_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T15:46:00.857341Z",
     "start_time": "2019-05-21T15:45:57.161249Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.reshape(X, (np.shape(X)[0], np.shape(X)[1], 1))\n",
    "y = np.reshape(y, (np.shape(y)[0], np.shape(y)[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y[0]))\n",
    "print(len(X[0]))\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos los datasets de entrenamiento, validacion y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T15:46:04.210282Z",
     "start_time": "2019-05-21T15:46:02.920731Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_rem, y_train, y_rem = train_test_split(df_data.mixed_rute,df_data.noise_rute, train_size=0.8)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
    "\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciamos el generador creado a mano y lo utilizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traingen=CustomDataGen(train,1920000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valgen=CustomDataGen(validate,1920000,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el generador de tensroflow de datos para que quepan todos con un mismo batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_dataset = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "trainy_dataset = tf.data.Dataset.from_tensor_slices((y_train))\n",
    "# train_dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "# val_dataset=tf.data.Dataset.from_tensor_slices((X_val,y_val))\n",
    "train_dataset = tf.data.Dataset.zip((trainx_dataset, trainy_dataset))\n",
    "valx_dataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
    "valy_dataset = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "val_dataset = tf.data.Dataset.zip((valx_dataset, valy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  probar la red con lo hecho ahora con los tensor slices\n",
    "np.shape(list(train_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T13:12:55.194805Z",
     "start_time": "2019-05-21T13:12:47.212092Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 1920000, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1920000, 64)  256         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 1920000, 64)  12352       conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 960000, 64)   0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 960000, 128)  24704       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 960000, 128)  49280       conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 480000, 128)  0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 480000, 256)  98560       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 480000, 256)  196864      conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 240000, 256)  0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 240000, 512)  393728      max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 240000, 512)  786944      conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 240000, 512)  0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 120000, 512)  0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 120000, 1024) 1573888     max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 120000, 1024) 3146752     conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 120000, 1024) 0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_4 (UpSampling1D)  (None, 240000, 1024) 0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 240000, 512)  1049088     up_sampling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 240000, 1024) 0           dropout_2[0][0]                  \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 240000, 512)  1573376     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 240000, 512)  786944      conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_5 (UpSampling1D)  (None, 480000, 512)  0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 480000, 256)  262400      up_sampling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 480000, 512)  0           conv1d_29[0][0]                  \n",
      "                                                                 conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 480000, 256)  393472      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 480000, 256)  196864      conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_6 (UpSampling1D)  (None, 960000, 256)  0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 960000, 128)  65664       up_sampling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 960000, 256)  0           conv1d_27[0][0]                  \n",
      "                                                                 conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 960000, 128)  98432       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 960000, 128)  49280       conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_7 (UpSampling1D)  (None, 1920000, 128) 0           conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 1920000, 64)  16448       up_sampling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1920000, 128) 0           conv1d_25[0][0]                  \n",
      "                                                                 conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 1920000, 64)  24640       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 1920000, 64)  12352       conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 1920000, 2)   386         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 1920000, 1)   3           conv1d_46[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,812,677\n",
      "Trainable params: 10,812,677\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sistema = platform.system()\n",
    "if (sistema ==\"Linux\"):\n",
    "    # Open a strategy scope.\n",
    "    with mirrored_strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "        # Call to the original u-Net\n",
    "        model = unet(inp)\n",
    "    adam=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['accuracy'])\n",
    "else:\n",
    "    model = unet(1920000)\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T00:19:39.394564Z",
     "start_time": "2019-05-21T16:35:21.065221Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "(1, 1920000)\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "path=\"./\"\n",
    "checkpoint = [callbacks.EarlyStopping(patience=5, restore_best_weights=True),callbacks.ModelCheckpoint(\n",
    "    path,\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"max\"\n",
    ")]\n",
    "\n",
    "history=model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss(history,1)\n",
    "\n",
    "pred_outp=model.predict(X_test)\n",
    "\n",
    "print(np.shape(pred_outp))\n",
    "audio_output = np.reshape(pred_outp, 1,(np.shape(pred_outp)))\n",
    "\n",
    "export_wav(pred_outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.173px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "823.991px",
    "left": "1470px",
    "right": "20px",
    "top": "109px",
    "width": "368.977px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
